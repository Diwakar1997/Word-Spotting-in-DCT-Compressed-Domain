{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PHOCNetProject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Diwakar1997/Word-Spotting-in-DCT-Compressed-Domain/blob/main/PHOCNetProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXNlw_mxx8pJ"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import tensorflow_addons\n",
        "# from keras.utils import multi_gpu_model\n",
        " \n",
        "from keras.models import Sequential, model_from_json\n",
        "from keras.layers import (Conv2D, MaxPooling2D, Dense, Dropout, Flatten,LeakyReLU, Activation)\n",
        " \n",
        "from keras.optimizers import SGD\n",
        "from keras import losses\n",
        "from keras.callbacks import TensorBoard\n",
        "from tensorflow_addons.layers import SpatialPyramidPooling2D\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPoVuNkMsdxM"
      },
      "source": [
        "!pip install tensorflow_addons"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzC5Jn6P88ms"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKyEdl1Sr7n4"
      },
      "source": [
        "'''This code will take an input word as in string and will\n",
        "output the PHOC label of the word. The Phoc label is a\n",
        "vector of length 604.\n",
        "'''\n",
        " \n",
        "def generate_36(word):\n",
        "  '''The vector is a binary and stands for:\n",
        "  [0123456789abcdefghijklmnopqrstuvwxyz]\n",
        "  '''\n",
        "  vector_36 = [0 for i in range(36)]\n",
        "  for char in word:\n",
        "    if char.isdigit():\n",
        "      vector_36[ord(char) - ord('0')] = 1\n",
        "    elif char.isalpha():\n",
        "      vector_36[10+ord(char) - ord('a')] = 1\n",
        " \n",
        "  return vector_36\n",
        " \n",
        "def generate_50(word):\n",
        "  '''This vector is going to count the number of most frequent\n",
        "  bigram words found in the text\n",
        "  '''\n",
        "  \n",
        "  bigram = ['th', 'he', 'in', 'er', 'an', 're', 'es', 'on', 'st', 'nt', 'en',\n",
        "  'at', 'ed', 'nd', 'to', 'or', 'ea', 'ti', 'ar', 'te', 'ng', 'al',\n",
        "  'it', 'as', 'is', 'ha', 'et', 'se', 'ou', 'of', 'le', 'sa', 've',\n",
        "  'ro', 'ra', 'hi', 'ne', 'me', 'de', 'co', 'ta', 'ec', 'si', 'll',\n",
        "  'so', 'na', 'li', 'la', 'el', 'ma']\n",
        "\n",
        "  vector_50 = [0 for i in range(50)]\n",
        "  for char in word:\n",
        "    try:\n",
        "      vector_50[bigram.index(char)] = 1\n",
        "    except:\n",
        "      continue\n",
        " \n",
        "  return vector_50\n",
        " \n",
        "def generate_label(word):\n",
        "  word = word.lower()\n",
        "  vector = []\n",
        "  L = len(word)\n",
        "  for split in range(2, 6):\n",
        "    parts = L//split\n",
        "    for mul in range(split-1):\n",
        "      vector += generate_36(word[mul*parts:mul*parts+parts])\n",
        "    vector += generate_36(word[(split-1)*parts:L])\n",
        " \n",
        "  vector += generate_50(word[0:L//2])\n",
        "  vector += generate_50(word[L//2: L])\n",
        "\n",
        "\n",
        "  return vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmuIMGyp93dh"
      },
      "source": [
        "'''This loads data in accordance to the standards mentioned in the IAM database.'''\n",
        "\n",
        "from glob import glob\n",
        "import cv2\n",
        "import math\n",
        "from xml.etree import ElementTree as ET\n",
        "from skimage import transform\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        " \n",
        "WORD_IMAGE_DIR = '/content/drive/My Drive/Project CNN/words/'\n",
        "XML_DIR = '/content/drive/My Drive/Project CNN/xml/'\n",
        "transcripts = {}\n",
        "block_size = 8\n",
        "\n",
        "QUANTIZATION_TABLE = np.array([[16,11,10,16,24,40,51,61],\n",
        "                             [12,12,14,19,26,58,60,55],\n",
        "                             [14,13,16,24,40,57,69,56 ],\n",
        "                             [14,17,22,29,51,87,80,62],\n",
        "                             [18,22,37,56,68,109,103,77],\n",
        "                             [24,35,55,64,81,104,113,92],\n",
        "                             [49,64,78,87,103,121,120,101],\n",
        "                             [72,92,95,98,112,100,103,99]])\n",
        "\n",
        "\n",
        "def apply_dct(img):\n",
        "\n",
        "    height, width = 50,100\n",
        "    h = height/block_size\n",
        "    w = width/block_size\n",
        "    h = np.int32(h)\n",
        "    w = np.int32(w)\n",
        "    final_out = np.zeros((height,width))\n",
        "    for i in range(h):\n",
        "        start_row = i*block_size\n",
        "        end_row = (i+1)*block_size\n",
        "        for j in range(w):\n",
        "            start_col = j*block_size\n",
        "            end_col = (j+1)*block_size\n",
        "            \n",
        "            block = np.float32(img[start_row:end_row,start_col:end_col])\n",
        "            \n",
        "            block_dct = cv2.dct(block)\n",
        "            \n",
        "            quan_dct = np.divide(block_dct,QUANTIZATION_TABLE).astype(int)*2\n",
        "            \n",
        "            final_out[start_row:end_row,start_col:end_col] = quan_dct\n",
        "    \n",
        "    return final_out\n",
        " \n",
        "def rule():\n",
        "    \"\"\"IAM Dataset has some set of rules against which we must compare\n",
        "    our models. We are loading those rules to set:\n",
        "    (Training_data, Validation_data, Test_data)\n",
        "    \"\"\"\n",
        "    with open('/content/drive/My Drive/Project CNN/rules/trainset.txt', 'r') as fp:\n",
        "        train_rule = fp.readlines()\n",
        "    train_rule = [x.strip() for x in train_rule]\n",
        " \n",
        "    with open('/content/drive/My Drive/Project CNN/rules/validationset1.txt', 'r') as fp: \n",
        "        valid_rule = fp.readlines()\n",
        "    with open('/content/drive/My Drive/Project CNN/rules/validationset2.txt', 'r') as fp: \n",
        "        valid_rule += fp.readlines()\n",
        "    valid_rule = [x.strip() for x in valid_rule]\n",
        " \n",
        "    with open('/content/drive/My Drive/Project CNN/rules/testset.txt', 'r') as fp: \n",
        "        test_rule = fp.readlines()\n",
        "    test_rule = [x.strip() for x in test_rule]\n",
        " \n",
        "    return train_rule, valid_rule, test_rule\n",
        " \n",
        " \n",
        "def append_data(x, y, transcript, data):\n",
        "    x.append(data[0])\n",
        "    y.append(data[1])\n",
        "    transcript.append(data[2])\n",
        " \n",
        " \n",
        "def load_data():\n",
        "    time_start = datetime.now()\n",
        " \n",
        "    train_rule, valid_rule, test_rule = rule()\n",
        " \n",
        "    xml_files = glob(XML_DIR+'*.xml')\n",
        " \n",
        "    x_train = []\n",
        "    y_train = []\n",
        "    train_transcript = []\n",
        "    x_valid = []\n",
        "    y_valid = []\n",
        "    valid_transcript = []\n",
        "    x_test = []\n",
        "    y_test = []\n",
        "    test_transcript = []\n",
        "    global transcripts\n",
        "    \n",
        "    br = 0\n",
        "    for xml_file in xml_files:\n",
        "        br += 1\n",
        "        if br == 150:\n",
        "            break\n",
        "        \n",
        "        print(\"Read Iteration = {}, time = {}\".format(br, datetime.now() - time_start))\n",
        "        tree = ET.parse(xml_file)\n",
        "        root = tree.getroot()\n",
        "       \n",
        "        image_dir = xml_file.split('/')[-1].split('.')[0].split('-')\n",
        "        image_dir = image_dir[0] + '/' + image_dir[0]+'-'+image_dir[1]+ '/'\n",
        "        image_dir = WORD_IMAGE_DIR + image_dir\n",
        " \n",
        "        for word in root.iter('word'):\n",
        "            img_id = word.get('id')\n",
        "            img_name = image_dir+img_id+'.png'\n",
        "            img_line = '-'.join(img_id.split('-')[:-1])\n",
        "            img_transcript = word.get('text').lower()\n",
        " \n",
        "            img = cv2.imread(img_name, 0)\n",
        "            if img is None:\n",
        "                continue\n",
        "            target = generate_label(img_transcript)\n",
        "            # if br == 1:\n",
        "            #     print(\"original word \", img_transcript)\n",
        "            #     print(\"phoc vector - \", target)\n",
        "\n",
        "            if sum(target) == 0: \n",
        "                img_transcript = ''\n",
        "            \n",
        "            img = cv2.resize(img, (100, 50))\n",
        "            # print()\n",
        "            # print(\"image before applying dct \")\n",
        "            print(np.ndarray.flatten(img).tolist())\n",
        "            #to dct\n",
        "            img = apply_dct(img)\n",
        "            # print(\"image after applying dct \")\n",
        "            # print(np.ndarray.flatten(img).tolist())\n",
        "            # print()\n",
        "            #end\n",
        "\n",
        "            img = cv2.resize(img, (100, 50))\n",
        "            img = np.where(img<200, 1, 0)\n",
        "            img = img[:, :, np.newaxis]\n",
        "            data = [img, target, img_transcript]\n",
        " \n",
        "            if br <= 100:\n",
        "                append_data(x_train, y_train, train_transcript, data)\n",
        "            elif br > 101 and br <= 125:\n",
        "                append_data(x_valid, y_valid, valid_transcript, data)\n",
        "            elif br > 125 and br <= 150:\n",
        "                append_data(x_test, y_test, test_transcript, data)\n",
        "            \n",
        "            # if img_line in train_rule:\n",
        "            #     append_data(x_train, y_train, train_transcript, data)\n",
        "            # elif img_line in valid_rule:\n",
        "            #     append_data(x_valid, y_valid, valid_transcript, data)\n",
        "            # elif img_line in test_rule:\n",
        "            #     append_data(x_test, y_test, test_transcript, data)\n",
        "\n",
        "\n",
        "    N = len(x_train) + len(x_valid) + len(x_test)\n",
        " \n",
        "    x_train = np.array(x_train)\n",
        "    y_train = np.array(y_train)\n",
        "    train_trainscript = np.array(train_transcript)\n",
        " \n",
        "    x_valid = np.array(x_valid)\n",
        "    y_valid = np.array(y_valid)\n",
        "    valid_transcript = np.array(valid_transcript)\n",
        " \n",
        "    x_test = np.array(x_test)\n",
        "    y_test = np.array(y_test)\n",
        "    test_transcript = np.array(test_transcript)\n",
        " \n",
        "    print (\"Time to fetch data: \", datetime.now() - time_start)\n",
        " \n",
        "    return (x_train, y_train, train_transcript,\n",
        "            x_valid, y_valid, valid_transcript,\n",
        "            x_test, y_test, test_transcript)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dk4N-Fwh4VC-"
      },
      "source": [
        "def test_model(model, x_test, y_test, transcripts):\n",
        "    start = datetime.now()\n",
        "    y_pred = model.predict(x_test)\n",
        "    y_pred = np.where(y_pred<0.5, 0, 1)\n",
        "    print(\"Time taken to predict \", datetime.now()-start)\n",
        "    count = 0\n",
        "    n = len(x_test)\n",
        "    for i in range(n):\n",
        "        pred = y_pred[i]\n",
        "        acc = np.sum(abs(y_test[i]-pred))\n",
        "        tmp = np.argmin(acc)\n",
        "        if transcripts[tmp] == transcripts[i]:\n",
        "            count += 1\n",
        "        print(\"Word = \", transcripts[i])\n",
        "        print(\"predicted phoc vector \")\n",
        "        print(pred)\n",
        "        print(\"predicted word = \",transcripts[tmp])\n",
        "        print()\n",
        "    precision = count/n\n",
        "    print(\"Precision = \", precision)\n",
        "\n",
        "    print(\"Total time taken = \", datetime.now()-start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvVpwM1eNQ07"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import keras\n",
        " \n",
        "from keras.models import Sequential, model_from_json\n",
        "from keras.layers import (Conv2D, MaxPooling2D, Dense, Dropout, Flatten, LeakyReLU, Activation)\n",
        "from keras.optimizers import SGD\n",
        "from keras import losses\n",
        "from keras.callbacks import TensorBoard\n",
        "from datetime import datetime\n",
        " \n",
        "def create_model():\n",
        "  \"\"\"This module creates an Instance of the Sequential Class in Keras.\n",
        "  Args:\n",
        "    None.\n",
        "  Return:\n",
        "    model: Instance of the Sequential Class\n",
        "  \"\"\"\n",
        "  time_start = datetime.now()\n",
        " \n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(64, (3, 3), padding='same',activation='relu', input_shape=(50, 100,1)))\n",
        "#   model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "#   model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "#   model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "  model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "  model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "  model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "  model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "  model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "  model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "  model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "  model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "  model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "  model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "  model.add(SpatialPyramidPooling2D([1,2,4]))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(4096, activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(4096, activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(604, activation='sigmoid'))\n",
        " \n",
        "  loss = losses.binary_crossentropy\n",
        "  optimizer = SGD(lr=1e-4, momentum=.9, decay=5e-5)\n",
        "#   optimizer = SGD(lr=1e-4)\n",
        "  model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  print (\"Time taken to create model: \", datetime.now()-time_start) \n",
        "    \n",
        "  return model\n",
        " \n",
        " \n",
        "def train():\n",
        " \n",
        "  time_start = datetime.now()\n",
        "  model = create_model()\n",
        " \n",
        "  data = load_data()\n",
        "  print(\"data_loaded\\n\")\n",
        "  x_train = data[0]\n",
        "  y_train = data[1]\n",
        "  x_valid = data[3]\n",
        "  y_valid = data[4]\n",
        "  x_test = data[6]\n",
        "  y_test = data[7]\n",
        "  test_transcripts = data[8]\n",
        " \n",
        "  model.fit(x_train,y_train,batch_size=10,epochs = 10,validation_data=(x_valid, y_valid))\n",
        "  train_time = datetime.now()-time_start\n",
        "  print (\"Time taken to train the model: \", datetime.now()-time_start)\n",
        "  test_model(model, x_test, y_test, test_transcripts)\n",
        " \n",
        "  print (\"Time taken to test the model: \", datetime.now()-train_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMDHKhj2M-hj"
      },
      "source": [
        "train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYGKtloTW2Kx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}